{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea80375-ec4b-406c-a3c2-f774f20e0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f32c30-ba54-46d4-9639-f77c418d6d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8a216-b3fe-4ad6-9d98-502e56498f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58f311-8e9d-4228-844f-ddff76b9f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        config (\"spark.sql.warehouse.dir\",\"/user/hive/warehouse\").\\\n",
    "        enableHiveSupport().\\\n",
    "        appName(\"Spark SQL - Data processing - Linear regression\").\\\n",
    "        master(\"yarn\").\\\n",
    "        getOrCreate()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86a822-1171-48ca-b13b-58470af4d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        config(\"spark.sql.warehouse.dir\",\"/user/hive/warehouse\").\\\n",
    "        enableHiveSupport().\\\n",
    "        appName(\"Exercise-01 | Get Monthly crime count by type \").\\\n",
    "        master(\"yarn\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed9f99-2f43-4d2d-b98a-217390dc6938",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b5cb4-74aa-46d8-94d0-28ce114a1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58085c42-0fe9-4fa0-a45e-56c664f8f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1882764-e18f-418f-b02f-488d99a0e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "create database hr_db\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec72247-09dc-4d6e-94ea-8e712c50fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "show databases\n",
    "\"\"\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160e3fe-9508-4357-8e8e-fd025c5b9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"use hr_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36985cb-5f68-433e-87be-082f5e96d6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select current_database()\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77023f8c-7a49-40c6-b042-2ebb3cabb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE employees (\n",
    "  employee_id     int,\n",
    "  first_name      varchar(20),\n",
    "  last_name       varchar(25),\n",
    "  email           varchar(25),\n",
    "  phone_number    varchar(20),\n",
    "  hire_date       date,\n",
    "  job_id          varchar(10),\n",
    "  salary          decimal(8,2),\n",
    "  commission_pct  decimal(2,2),\n",
    "  manager_id      int,\n",
    "  department_id   int\n",
    ") ROW FORMAT \n",
    "    DELIMITED FIELDS TERMINATED BY '\\t'\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8687d2e6-3875-46dc-9b6a-b6a00b9f9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "Load data LOCAL inpath '/home/forgcpmak/hr_db/employees' INTO TABLE hr_db.employees\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36131185-77ec-4b77-b047-a56804905da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select * from hr_db.employees\n",
    "\"\"\").show(25,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896aa98c-e95d-4318-8856-e85647e060af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql (\"\"\"\n",
    "SELECT employee_id, \n",
    "       department_id, \n",
    "       salary,\n",
    "       count(1) OVER (PARTITION BY department_id) AS employee_count,\n",
    "       rank() OVER (ORDER BY salary DESC) AS rnk,\n",
    "       lead(employee_id,1,0) OVER (PARTITION BY department_id ORDER BY salary desc)  lead_emp_id,\n",
    "       lead(salary,1,0) OVER (PARTITION BY department_id ORDER BY salary desc) AS lead_emp_sal\n",
    "FROM employees\n",
    "ORDER BY employee_id\n",
    "\"\"\").show(3,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84345f41-12ab-47b6-838e-ee9f38674f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql (\"\"\"\n",
    "SELECT e.employee_id, e.department_id, e.salary,\n",
    "       sum(e.salary) \n",
    "         OVER (PARTITION BY e.department_id)\n",
    "         AS department_salary_expense\n",
    "FROM employees e\n",
    "ORDER BY e.department_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d10ebc-3ad2-4af5-a13a-23daacbcee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE retail_db.daily_revenue\n",
    "AS\n",
    "SELECT o.order_date,\n",
    "       round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM retail_db.orders o \n",
    "\n",
    "JOIN retail_db.order_items oi\n",
    "ON o.order_id = oi.order_item_order_id\n",
    "\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date\n",
    "\"\"\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c305d3-f41b-44db-9ec5-9ddcb69888af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "drop table if exists retail_db.daily_revenue\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b47d523-ce5e-48a0-8985-a98a8305e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT t.*,\n",
    "  first_value(order_item_product_id) OVER (\n",
    "    PARTITION BY order_date ORDER BY revenue DESC\n",
    "  ) first_product_id,\n",
    "  first_value(revenue) OVER (\n",
    "    PARTITION BY order_date ORDER BY revenue DESC\n",
    "  ) first_revenue\n",
    "FROM daily_product_revenue t\n",
    "ORDER BY order_date, revenue DESC\n",
    "LIMIT 100\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a8ff01-5a8c-4a08-8cd7-5da1797dacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "help (spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d5773-396b-418f-af16-52cab30ca4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a2378-c43f-48be-8d78-9055b0d31153",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    csv('/user/forgcpmak/retail_db/orders',\n",
    "        header=False,\n",
    "        schema='''\n",
    "            order_id INT, \n",
    "            order_date STRING, \n",
    "            order_customer_id INT, \n",
    "            order_status STRING\n",
    "        '''\n",
    "       ). \\ \n",
    "    show(25,truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e38a3-7229-4cce-838c-b25b21292471",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_schema = spark. \\\n",
    "       read. \\\n",
    "    csv('/user/forgcpmak/data/data/hr_db/employees',\n",
    "        header= False,\n",
    "        inferSchema=True\n",
    "       ).\\\n",
    "    schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e72124-f81b-4538-a51d-abd257cc26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(employee_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2f6ae-7ce6-450d-9b2d-cbac002b5505",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e90d1-2e4f-409b-9c19-154641d79c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2da0a-8206-46da-924f-a8af64cb8d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDF = spark.read. \\\n",
    "     csv('/user/forgcpmak/data/data/hr_db/employees',\n",
    "        schema=employee_schema,\n",
    "        header=False\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d62ee-c68c-4a47-8336-32e6bed9e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2bd695-2118-4758-bf0a-719fe13e63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4d43a-10c3-4764-9c6d-ad7cf960e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2da8cc-7f99-4aff-b6a6-ce55f514610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c017dd4-8554-49d9-87eb-9bdd2a85fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2406c2db-3ca1-415f-a31c-4b03ae94a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(SparkContext.textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3360c61-a907-44fe-af01-c5d795059067",
   "metadata": {},
   "outputs": [],
   "source": [
    "employeeDF = spark.read.text(\"/user/forgcpmak/data/data/hr_db/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3e330-5107-422a-a50f-f9c50622bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDf=spark.read.\\\n",
    "options(delimiter=';',inferSchema = True , header = True).\\\n",
    "csv(\"/user/forgcpmak/data/Crimes_-_2001_to_Present.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5d3c0-3679-40f7-8373-7a56e6e08bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDf = crimeDf\\\n",
    "              .withColumnRenamed('Primary Type','PrimaryType')\\\n",
    "              .withColumnRenamed('FBI Code','FBICode')\\\n",
    "              .withColumnRenamed('X Coordinate','X_Coordinate')\\\n",
    "              .withColumnRenamed('Y Coordinate','Y_Coordinate')\n",
    "\n",
    "crimeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224d61d-03a7-4ba8-8833-bf0fef1c05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important Learning here is - the format provided to to_date function - 'MM/dd/yyyy' , should match with data of date being read from the file.\n",
    "crimeDf.\\\n",
    "      select('Date','PrimaryType').\\\n",
    "      withColumn('DateStringConvertedToDate' , to_date(substring(col('Date'),1,10),'MM/dd/yyyy')).\\\n",
    "      withColumn('Month' , date_format(to_date(substring(col('Date'),1,10),'MM/dd/yyyy'),'yyyyMM')).\\\n",
    "      show(2, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd87be-8189-4a2e-bce2-3f53efb74536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While saving file with zip compressions , need to use th codec in options.\n",
    "crimeDf.\\\n",
    "      withColumn('Month' , date_format(to_date(substring(col('Date'),1,10),'MM/dd/yyyy'),'yyyyMM')).\\\n",
    "      groupBy(col('Month'),col('PrimaryType')).\\\n",
    "      agg (\n",
    "         count(lit(1)).cast('int').alias('CrimeCount')\n",
    ").\\\n",
    "orderBy(col('Month').asc(),col(('CrimeCount')).desc()).\\\n",
    "repartition(1).\\\n",
    "write.\\\n",
    "format('csv').\\\n",
    "mode('overwrite').\\\n",
    "options(sep ='\\\\t', header= True, codec = \"org.apache.hadoop.io.compress.GzipCodec\").\\\n",
    "save(\"/user/forgcpmak/data/CrimeCountByMonth/CrimeCountByMonth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001cdf0c-db83-40d1-ab88-0a0a389a6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving file in ORC format \n",
    "crimeDf.\\\n",
    "      withColumn('Month' , date_format(to_date(substring(col('Date'),1,10),'MM/dd/yyyy'),'yyyyMM')).\\\n",
    "      groupBy(col('Month'),col('PrimaryType')).\\\n",
    "      agg (\n",
    "         count(lit(1)).cast('int').alias('CrimeCount')\n",
    "       ).\\\n",
    "      select ( col('Month'),col('PrimaryType').alias('PrimaryType'),col('CrimeCount') ).\\\n",
    "      orderBy(col('Month').asc(),col(('CrimeCount')).desc()).\\\n",
    "      repartition(1).\\\n",
    "      write.\\\n",
    "      mode('overwrite').\\\n",
    "      save(\"/user/forgcpmak/data/CrimeCountByMonthOrc\",format=\"orc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216d036c-94c6-4c86-b079-7c869db1b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving file in AVRO does not work in SPARK a it needs additional libraries\n",
    "crimeDf.\\\n",
    "      withColumn('Month' , date_format(to_date(substring(col('Date'),1,10),'MM/dd/yyyy'),'yyyyMM')).\\\n",
    "      groupBy(col('Month'),col('Primary Type')).\\\n",
    "      agg (\n",
    "         count(lit(1)).cast('int').alias('CrimeCount')\n",
    "       ).\\\n",
    "      select ( col('Month'),col('Primary Type').alias('PrimaryType'),col('CrimeCount') ).\\\n",
    "      orderBy(col('Month').asc(),col(('CrimeCount')).desc()).\\\n",
    "      repartition(1).\\\n",
    "      write.\\\n",
    "      mode('overwrite').\\\n",
    "      save(\"/user/forgcpmak/data/CrimeCountByMonthAvro\",format=\"avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49dcb4-b4f4-4f68-8a7b-75743b352754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#While saving file in specific format - ORC/AVRO - either use - method. parquet/orc or pass option as format='ORC'/'parquet' in save method\n",
    "crimeDf.\\\n",
    "      withColumn('Month' , date_format(to_date(substring(col('Date'),1,10),'MM/dd/yyyy'),'yyyyMM')).\\\n",
    "      groupBy(col('Month'),col('Primary Type')).\\\n",
    "      agg (\n",
    "         count(lit(1)).cast('int').alias('CrimeCount')\n",
    "       ).\\\n",
    "      select ( col('Month'),col('Primary Type').alias('PrimaryType'),col('CrimeCount') ).\\\n",
    "      orderBy(col('Month').asc(),col(('CrimeCount')).desc()).\\\n",
    "      repartition(1).\\\n",
    "      write.\\\n",
    "      mode('overwrite').\\\n",
    "      parquet(\"/user/forgcpmak/data/CrimeCountByMonthParq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce8304-7394-49dd-85e3-6d36cf760c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COnvert the Date value which is string in source file to date using to_date by matching the format in input file and then on that converted date apply the date format to \n",
    "#extract the month.\n",
    "crimeCountByMonthDF = crimeDf.\\\n",
    "      withColumn('Month' , date_format(to_date(substring(col('Date'),1,10),'MM/dd/yyyy'),'yyyyMM')).\\\n",
    "      groupBy(col('Month'),col('PrimaryType')).\\\n",
    "      agg (\n",
    "         count(lit(1)).alias('CrimeCount')\n",
    ").\\\n",
    "orderBy(col('Month').asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddecc3d8-8447-45c6-9eab-57b3dab44aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeCountByMonthDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5e516-16bf-4ab2-8709-e624a035912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demo on how to use RANK function.\n",
    "\n",
    "#Step #1 - Import the Window \n",
    "from pyspark.sql.window import Window\n",
    "#Stpe #2 - Import the rank function\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "#Step #3 - Create the window spec. \n",
    "#In this example - aim is to partition by PrimaryType  and order byy CrimeCount and then list in descending order.\n",
    "#SO that records with rank = 1 can give the max of given PrimaryType.\n",
    "windowSpec  = Window.partitionBy(\"PrimaryType\").orderBy(col(\"CrimeCount\").desc())\n",
    "\n",
    "#Step #4 - Apply the window spec in using rank() funtion.\n",
    "crimeCountByMonthDF.\\\n",
    "                  withColumn(\"rank\",rank().over(windowSpec)).\\\n",
    "                  filter( (crimeCountByMonthDF['PrimaryType'] == 'PROSTITUTION') ).\\\n",
    "                  filter(col('rank') == 1 ).\\\n",
    "                  show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afe1ce-2586-4486-b2aa-44271094c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "highestCrimeMonthsCategoryWiseDf = crimeCountByMonthDF.\\\n",
    "                  withColumn(\"rank\",rank().over(windowSpec)).\\\n",
    "                  filter(col('rank') == 1 ).\\\n",
    "                  drop('rank').\\\n",
    "                  orderBy(col('CrimeCount').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a6f45-7058-4710-a1ad-0222eee28533",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeDf.\\\n",
    "       withColumn ( 'CrimeMmonth' , date_format('Date','yyyyMM') ).\\\n",
    "       select('Date','Primary Type','CrimeMmonth').\\\n",
    "       show(2, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55cdb9-bbf7-4b5e-89f8-f74bf5afa85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "highestCrimeMonthsCategoryWiseDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e0f0a-421e-43b6-9395-59be729eb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading tab delimited file.\n",
    "df3 = spark.read.options(delimiter='\\\\t') \\\n",
    "     .schema (\"\"\"\n",
    "                Id int , \\\n",
    "                FirstName String , \\\n",
    "                LastName String, \\\n",
    "                ShortName String, \\\n",
    "                MobileNumber String ,\\\n",
    "                DateofBirth Date, \\\n",
    "                Department string,\n",
    "                Salary Double,\n",
    "                HikePct Double,\n",
    "                ManagerId Int,\n",
    "                DepatMentId Int\n",
    "             \"\"\")\\\n",
    "     .csv(\"/user/forgcpmak/data/data/hr_db/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f158a-6258-4227-a97b-507be576e7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dbf5cf-b93a-456f-8edd-656de8af2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.read.options(delimiter='\\\\t',inferSchema = True) \\\n",
    "     .csv(\"/user/forgcpmak/data/data/hr_db/employees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ca11d-11ca-43ce-afa1-4bc4a2d1ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36ccac-0a92-43dd-8648-e4569d7fcdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.filter (\"ManagerId is null\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b0db1-f21d-4119-bf23-cb80e84af5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420f543-140c-4913-aae8-9a9e705df8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.filter (df3['ManagerId'] == 'null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6a293-c787-4951-a930-0f5c982153cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.withColumn ( \"BirthDayName\",date_format(col(\"DateofBirth\"),'EEEE') )\\\n",
    "   .filter(\"( date_format(DateofBirth,'EEEE') = 'Sunday')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8e58f-4517-4cab-9c44-0b56778a8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.select(countDistinct(df3['Department'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d552908-18fb-48a9-822f-36e76543a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.filter(\"FirstName like 'D%'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178356d8-059a-457e-800e-89dc3b7504ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.filter( col(\"FirstName\").like(\"D%\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a528d1c-02a9-4ca7-9fa8-c9c07d0de789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.groupBy(df3['Department']).count().sort(col('Department'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908f195-e6dd-4e96-abfa-656d9509cc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.filter('HikePct is not null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9f94d-a7f8-42b4-8f6e-3c607b7b386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.filter( (df3['HikePct'].isNotNull()) & \\\n",
    "             (df3['HikePct'] > 0.0)\\\n",
    "            ).\\\n",
    "          groupBy(df3['Department'],df3['HikePct']).\\\n",
    "          agg( \n",
    "               count(lit(1)).alias(\"TotalCount\"),\n",
    "               round(avg(df3['HikePct']),2).alias(\"AvgHikePct\")\n",
    "               ).\\\n",
    "          sort(col('AvgHikePct').desc()).\\\n",
    "          show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9156fe94-6b7d-47d8-b781-d8420eaa15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.filter( (df3['HikePct'].isNotNull()) & \\\n",
    "             (df3['HikePct'] > 0.2)\\\n",
    "            ).\\\n",
    "          rollup(df3['Department'],df3['HikePct']).\\\n",
    "          agg( \n",
    "               count(lit(1)).alias(\"TotalCount\"),\n",
    "               round(avg(df3['HikePct']),2).alias(\"AvgHikePct\")\n",
    "               ).\\\n",
    "          sort(col('Department').asc(),col('HikePct').asc()).\\\n",
    "          show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41d337-cadf-4311-a110-5cf48fb78f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3. \\\n",
    "    select(\n",
    "            (\n",
    "              ( sum (\n",
    "                      coalesce(\n",
    "                                col('HikePct').cast('int'), lit(0)\n",
    "                             )\n",
    "                          * col('Salary')\n",
    "                     )\n",
    "              ) / lit(100)\n",
    "            ).alias('total_bonus')\n",
    "           ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f37427-2027-4189-8e4f-afe2a52b6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3. \\\n",
    "    selectExpr('sum((coalesce(cast(HikePct AS INT), 0) * Salary) / 100) AS total_bonus'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d1856a-3e93-4732-b8c3-43ff8e528d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3. \\\n",
    "    select('Department', 'Salary', 'HikePct'). \\\n",
    "    describe(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a87ad-cb95-4b21-a27c-1f19be2d5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3. \\\n",
    "    select('Department', 'Salary', 'HikePct'). \\\n",
    "    summary(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6322b-f948-4dd1-a5d0-5a44f1e8b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b61165-ecb3-4139-a37e-a7d2b5eba06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12041c94-6915-4822-83fa-0dce8a248d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c4414-ecac-4fa2-ae44-060c2eeb64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(eomployeeDFSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c343ca8-4dfa-4d16-9966-2fa4dfdf1b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = col('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab21dbb-876e-4a6b-bf8f-7a630237e50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f309abff-c5bd-40c8-9b37-647b8b7070cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf = spark.read.json('/user/forgcpmak/data/data/retail_db_json/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2733bd8-b6fd-4141-a4b2-f421469f3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449bf13-3438-42c8-bf04-a96c58b8c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6e0e8-ee1f-447d-92d6-ad496eb2bbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c2103-3ef3-4bed-89ea-d2d724ee03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf.\\\n",
    "         groupBy(col('order_date') ).\\\n",
    "         agg (\n",
    "                count(lit(1)).alias('order_count_by_date')\n",
    ").sort(col('order_date').asc()).\\\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6569790-c2fe-4062-b962-c43f9f80ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf.\\\n",
    "         cube(col('order_date')).\\\n",
    "         agg (\n",
    "                count(lit(1)).alias('order_count_by_date')\n",
    ").orderBy(col('order_date').asc()).\\\n",
    "show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb806e4-0442-4759-a9c2-9abe09e30e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf.\\\n",
    "         cube(col('order_date')).\\\n",
    "         agg (\n",
    "                count(lit(1)).alias('order_count_by_date')\n",
    ").sort(col('order_date').asc()).\\\n",
    "count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c91229b-2752-4320-952a-0969d324e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf. \\\n",
    "    groupBy(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format('order_date', 'yyyyMM').alias('order_month'), \n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg(count(lit(1)).alias('order_count')). \\\n",
    "    orderBy('order_year', 'order_month', 'order_date'). \\\n",
    "    show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19047be6-ea50-4d98-9074-be5f1f959e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21476b-e400-4eb4-99fc-ad21aee9ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf. \\\n",
    "    groupBy(\n",
    "        year('order_date').alias('order_year'),\n",
    "        date_format('order_date', 'yyyyMM').alias('order_month'), \n",
    "        'order_date'\n",
    "    ). \\\n",
    "    agg( \n",
    "         count(lit(1)).alias('order_count') ,\n",
    "         sum( when(col('order_status') == 'CLOSED',1).otherwise(lit(0))).alias(\"ClosedOrderCount\"),\n",
    "         sum( when(col('order_status') == 'COMPLETE',1).otherwise(lit(0))).alias(\"CompletedOrderCount\") \n",
    "         ). \\\n",
    "        show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc1b70-fe9b-495c-a0bb-4c4abca8b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df = spark.read.json('/user/forgcpmak/data/data/retail_db_json/order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7960d-bdc2-4dd5-8294-60e2a2d47009",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = spark.read.json('/user/forgcpmak/data/data/retail_db_json/customers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e79244-fa86-43f8-b9aa-07aeea2c0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d6e8b-4bc0-456c-b544-3439d4c73eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers. \\\n",
    "    alias('c'). \\\n",
    "    join(\n",
    "        ordersDf.alias('o'), \n",
    "        on=customers['customer_id'] == ordersDf['order_customer_id'],\n",
    "        how='left'\n",
    "    ). \\\n",
    "    filter('o.order_id IS NULL'). \\\n",
    "    selectExpr('c.customer_id', 'c.customer_email', 'o.*'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8801d-a3c0-4cc5-af32-57b73fa7a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers. \\\n",
    "    alias('c'). \\\n",
    "    join(\n",
    "        ordersDf.alias('o'), \n",
    "        on=customers['customer_id'].eqNullSafe(ordersDf['order_customer_id']),\n",
    "        how='left'\n",
    "    ). \\\n",
    "    select (customers['customer_id'],ordersDf['order_id'])\\\n",
    "    .groupBy(customers['customer_id'])\\\n",
    "    .agg (\n",
    "           sum( when (col('order_id').isNull(),0).otherwise(lit(1))).alias('order_count')\n",
    "         )\\\n",
    "    .orderBy(col('order_count').desc())\\\n",
    "    .filter (col('order_count') == 0)\\\n",
    "    .show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c42d9-0658-4852-b462-e1d262dd81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Get the number of orders placed by each customer for the year 2013. If a customer have not placed any order get the order count for the customer as 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0752dda-3d5d-4e5c-96b5-9a99bacba768",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b156777-c6e7-442a-a53c-92d05fc6b858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing equal column - in null safe manner using. - eqNullSafe \n",
    "ordersDf. \\\n",
    "    join(\n",
    "        order_items_df, \n",
    "        on=ordersDf['order_id'].eqNullSafe(order_items_df['order_item_order_id']),\n",
    "        how='inner'\n",
    "    ). \\\n",
    "    select(ordersDf['order_id'], ordersDf['order_date'], ordersDf['order_status'], order_items_df['order_item_subtotal']). \\\n",
    "    filter ( ordersDf ['order_status'] == 'CLOSED' ).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4c6ac-f6ae-422b-9015-ed5863c85658",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf. \\\n",
    "    join(\n",
    "        order_items_df, \n",
    "        on=ordersDf['order_id'] == order_items_df['order_item_order_id'],\n",
    "        how='inner'\n",
    "    ). \\\n",
    "    select(ordersDf['*'],order_items_df['*']). \\\n",
    "    filter ( ordersDf ['order_status'] == 'CLOSED' ).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be9e89-db8b-47d6-abe7-27b5b17984f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items_df.\\\n",
    "               groupBy(order_items_df['order_item_product_id']).\\\n",
    "               agg (\n",
    "                   count(lit(1)).alias(\"NumberOfTimeOrdered\")  \n",
    ").\\\n",
    "orderBy(col('NumberOfTimeOrdered').desc()).\\\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121150a2-5e3e-4295-b01a-6ab2a8c015df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf. \\\n",
    "    filter(\"order_status IN ('COMPLETE', 'CLOSED')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06e942-7cc5-4716-8940-13c1852d8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf.\\\n",
    "         filter( ordersDf['order_status'].isin('COMPLETE', 'CLOSED') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f86737-a0f9-4116-bd77-9cab8382c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDf.filter((\"order_status\" == 'COMPLETE') || (\"order_status\" == 'CLOSED'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c1872-85cc-4a4e-8d83-c6aebc75d7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "squaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n",
    "                                  .map(lambda i: Row(single=i, double=i ** 2)))\n",
    "squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "\n",
    "# Create another DataFrame in a new partition directory,\n",
    "# adding a new column and dropping an existing column\n",
    "cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "cubesDF.write.parquet(\"data/test_table/key=2\")\n",
    "\n",
    "# Read the partitioned table\n",
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\n",
    "mergedDF.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1fd310-2652-4bd9-a108-6e670d0b5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144789a-d151-47e4-a994-5d15883dd410",
   "metadata": {},
   "outputs": [],
   "source": [
    "squaresDF = spark.createDataFrame(sc.parallelize(range(1, 6))\n",
    "                                  .map(lambda i: Row(single=i, double=i ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6b38b-6182-4c41-8e90-53c24c843744",
   "metadata": {},
   "outputs": [],
   "source": [
    "squaresDF.write.mode('overwrite').save(\"/user/forgcpmak/data/test_table/key=1\",format = 'parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfb6ed-4874-46ce-96de-d44e9a841e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                .map(lambda i: Row(single=i, triple=i ** 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726d277-c7ee-4b75-b1b2-634e6e3d089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "squaresDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888016e6-e913-4d04-9644-017e0869ee8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubesDF.write.mode('overwrite').parquet(\"/user/forgcpmak/data/test_table/key=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657bec9-1c10-482a-9188-b11d73219bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"/user/forgcpmak/data/test_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5a78e-eb41-470b-b861-4f0bd7c44913",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9cfcdc-a83a-4c8d-aa8d-7f9b70249940",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ba03a-210b-47d1-bd68-e1463662c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"/user/forgcpmak/data/sales_info.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"True\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b189488-5a5a-4ffa-9acb-0e4e6aed7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bf1e4d-bc4d-499c-91b9-f0439466affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\\\n",
    "   .groupBy(df['Person'])\\\n",
    "   .agg(sum('Sales').alias('SumSalesByCompany'))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da8509-b1d9-41a5-97d6-9f515125ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec  = Window.partitionBy(\"Person\").orderBy(col(\"Company\").desc())\n",
    "df.\\\n",
    "  withColumn(\"rank\",rank().over(windowSpec)).\\\n",
    "  filter(col('rank') == 1 ).\\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac953f-200b-4c94-a627-17803aeebe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"/user/forgcpmak/data/appl_stock.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"True\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df_appl_stock = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4f11b-38ac-4d5b-8064-89edec17de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appl_stock.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7abff-dfe1-4fb2-9077-bb85619a43b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_appl_stock.describe().select('summary',format_number(col('open').cast('int'),2).alias('Open'),'high').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a505fd-c50b-4c0d-a989-82a08a3a8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e8cd66-629b-4d51-85ae-b1ce448395d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StructField, StructType, StringType, IntegerType, DecimalType\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff313556-74be-45b6-8c5a-f825aef4ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_stock_schema = StructType([\n",
    "                            StructField(\"Date\",DateType(),True)\n",
    "                           ,StructField(\"Open\",DoubleType(),True)\n",
    "                           ,StructField(\"High\",DoubleType(),True)\n",
    "                           ,StructField(\"Low\",DoubleType(),True)\n",
    "                           ,StructField(\"Close\",DoubleType(),True)\n",
    "                           ,StructField(\"Volume\",LongType(),True)\n",
    "                           ,StructField(\"AdjClose\",DoubleType(),True)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9aa77-b366-4e49-8bcc-c98906ea1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_location = \"/user/forgcpmak/data/walmart_stock.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"False\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "walmart_stock_df = spark.read.format(file_type) \\\n",
    "  .schema(walmart_stock_schema)\\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ac0b6-7333-4af6-9b8b-472ba1d2e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_stock_df.printSchema()\n",
    "walmart_stock_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc6a302-df3b-4292-8ff0-13836c759ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to RDD\n",
    "# RDD trasnformation is not requird.\n",
    "\n",
    "rddWalmartHigh = spark.sparkContext.parallelize(walmart_stock_df.orderBy(col('High').desc()).head(2))\n",
    "rddWalmartHigh.take(1)\n",
    "#Pass directly the list to createDataFrame method.\n",
    "walmartHighDf = spark.createDataFrame(spark.sparkContext.parallelize(walmart_stock_df.orderBy(col('High').desc()).head(2)),walmart_stock_schema)\n",
    "walmartHighDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a728f-2809-4b95-982a-80fa0c904910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use schema from existing DF.\n",
    "existingDFSchema = StructType.fromJson (walmartHighDf.schema.jsonValue())\n",
    "\n",
    "existingDFSchema = walmartHighDf.schema\n",
    "# use this schema now to read new DF.\n",
    "walmart_stock_read_from_df1 = spark.read.format(file_type) \\\n",
    "  .schema(existingDFSchema)\\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684a88aa-ce52-4a6b-b2e8-36ea217e1783",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_stock_read_from_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc9a4d-f864-4c2a-886d-140fad87f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the schema\n",
    "with open(\"/home/forgcpmak/schema.json\", \"w\") as f:\n",
    "    json.dump(walmartHighDf.schema.jsonValue(), f)\n",
    "\n",
    "# Read the schema\n",
    "with open(\"/home/forgcpmak/schema.json\") as f:\n",
    "    new_schema = StructType.fromJson(json.load(f))\n",
    "    print(new_schema.simpleString())\n",
    "    display(new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b09fa0d-e000-4595-99b7-4b2865ee9400",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f11f6-c9e4-4eb2-b0c3-53d31882b9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198c6fe4-1126-4c35-a854-011140d583bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_stock_df\\\n",
    "  .groupBy(col('Date'))\\\n",
    "  .agg(\n",
    "       format_number(max(col('Open')),2).alias('MaxOpenPrice')\n",
    "       )\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bac906-4124-47a5-aedc-077aedd276d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "appl_stock_window  = Window.partitionBy(\"Date\").orderBy(col(\"Open\").desc())\n",
    "walmart_stock_df.\\\n",
    "  select ('Date','Open').\\\n",
    "  withColumn(\"rank\",rank().over(appl_stock_window)).\\\n",
    "  filter(col('rank') == 1 ).\\\n",
    "  show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3519dad-8b77-45ce-aa4f-e6e55ffdbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_open_price= walmart_stock_df.agg({'open':'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd643aa-59ac-49ce-86ca-4844f4492563",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_open_price = max_open_price.withColumnRenamed('max(open)','MaxOpen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e741b-b914-4ac2-b29c-0be4e8b8bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType(\n",
    "    [StructField(\"name\", StringType(), True), StructField(\"age\", IntegerType(), True)]\n",
    ")\n",
    "\n",
    "# Write the schema\n",
    "with open(\"/home/forgcpmak/schema.json\", \"w\") as f:\n",
    "    json.dump(schema.jsonValue(), f)\n",
    "\n",
    "# Read the schema\n",
    "with open(\"/home/forgcpmak/schema.json\") as f:\n",
    "    new_schema = StructType.fromJson(json.load(f))\n",
    "    print(new_schema.simpleString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3ad9b-4f8a-4df4-bced-79ceb9369779",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(new_schema)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e3464-6d24-4ffa-a0f0-e985f01dddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the schema\n",
    "with open(\"/home/forgcpmak/schema.json\") as f:\n",
    "    new_schema = StructType.fromJson(json.load(f))\n",
    "    print(new_schema.simpleString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe0493d-0da5-4539-b476-7a58b78c5283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List\n",
    "data = [('Category A', 100, \"This is category A\"),\n",
    "        ('Category B', 120, \"This is category B\"),\n",
    "        ('Category C', 150, \"This is category C\")]\n",
    "\n",
    "# Create a schema for the dataframe\n",
    "schema = StructType([\n",
    "    StructField('Category', StringType(), True),\n",
    "    StructField('Count', IntegerType(), True),\n",
    "    StructField('Description', StringType(), True)\n",
    "])\n",
    "\n",
    "columns = [\"Category\", \"Count\" , \"Desc\" ]\n",
    "# Convert list to RDD\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# Create data frame\n",
    "df = spark.createDataFrame(data,columns)\n",
    "print(df.schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebdeb1-7b71-4e31-9be6-da86f38dd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Readind fixed width file \n",
    "#Based on pred defined columns positions \n",
    "\n",
    "#Step1 - defind the column name / data co-ordinates / data type in a list of tupesl.\n",
    "schema = [\n",
    "          (\"id\",1,5,\"int\"),\n",
    "          (\"ssn\",6,10,\"string\"),\n",
    "          (\"name\",16,4,\"string\")\n",
    "]\n",
    "          \n",
    "\n",
    "#Step#2. - Read the source file \n",
    "df = spark.read.text(\"/user/forgcpmak/data/personInfo.txt\")\n",
    "df.show()\n",
    "\n",
    "#Steps#3 - from source DF. , iterate over the schema list and re-attach the each separation in main DF.\n",
    "df2 = df\n",
    "#This loop will work on eveloving DF2 in an iterative manner for each column defined in schema list.\n",
    "for colinfo in schema:\n",
    "  df2 = df2.withColumn(colinfo[0], df2.value.substr(colinfo[1],colinfo[2]).cast(colinfo[3]))\n",
    "  # this print will demonstrate how the schema is evolving.\n",
    "  df2.show()\n",
    "\n",
    "df2 = df2.drop('value')\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d17a23-8db0-44af-84d1-8e03699ba8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below is code of Machine learning linear regressions\n",
    "\n",
    "# Read file. : /user/forgcpmak/data/josportillacource/Spark_for_Machine_Learning/Linear_Regression/cruise_ship_info.csv\n",
    "\n",
    "file_location = \"/user/forgcpmak/data/josportillacource/Spark_for_Machine_Learning/Linear_Regression/cruise_ship_info.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"True\"\n",
    "first_row_is_header = \"True\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "cruise_ship_info_df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2acb21-e5cc-4763-81d9-7379743528c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cruise_ship_info_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134dc473-9c14-45ac-a06f-13274c1a57ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ship_name: string (nullable = true)\n",
      " |-- Cruise_line: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Tonnage: double (nullable = true)\n",
      " |-- passengers: double (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- cabins: double (nullable = true)\n",
      " |-- passenger_density: double (nullable = true)\n",
      " |-- crew: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cruise_ship_info_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674f292d-cc1d-4069-83cc-45163563809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|Ship_name  |Cruise_line|Age|Tonnage           |passengers|length|cabins|passenger_density|crew|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|Journey    |Azamara    |6  |30.276999999999997|6.94      |5.94  |3.55  |42.64            |3.55|\n",
      "|Quest      |Azamara    |6  |30.276999999999997|6.94      |5.94  |3.55  |42.64            |3.55|\n",
      "|Celebration|Carnival   |26 |47.262            |14.86     |7.22  |7.43  |31.8             |6.7 |\n",
      "|Conquest   |Carnival   |11 |110.0             |29.74     |9.53  |14.88 |36.99            |19.1|\n",
      "|Destiny    |Carnival   |17 |101.353           |26.42     |8.92  |13.21 |38.36            |10.0|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cruise_ship_info_df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0759d55-b9b8-4883-8964-9fbd0a091942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------------+\n",
      "|     Cruise_line|ShipCountByrCruiseLine|\n",
      "+----------------+----------------------+\n",
      "| Royal_Caribbean|                    23|\n",
      "|        Carnival|                    22|\n",
      "|        Princess|                    17|\n",
      "|Holland_American|                    14|\n",
      "|       Norwegian|                    13|\n",
      "+----------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cruise_ship_info_df\\\n",
    "           .groupBy('Cruise_line')\\\n",
    "           .agg(\n",
    "             count(lit(1)).alias('ShipCountByrCruiseLine')\n",
    "             )\\\n",
    "            .orderBy(col('ShipCountByrCruiseLine').desc())\\\n",
    "            .show(5)\n",
    "           \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf6f6b22-8a21-4cc6-b7e2-8b7610dd6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "## Required to install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a22198bc-a781-470a-9f94-47f57f0ad09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Ship_name='Journey', Cruise_line='Azamara', Age=6, Tonnage=30.276999999999997, passengers=6.94, length=5.94, cabins=3.55, passenger_density=42.64, crew=3.55, cruise_cat=16.0),\n",
       " Row(Ship_name='Quest', Cruise_line='Azamara', Age=6, Tonnage=30.276999999999997, passengers=6.94, length=5.94, cabins=3.55, passenger_density=42.64, crew=3.55, cruise_cat=16.0),\n",
       " Row(Ship_name='Celebration', Cruise_line='Carnival', Age=26, Tonnage=47.262, passengers=14.86, length=7.22, cabins=7.43, passenger_density=31.8, crew=6.7, cruise_cat=1.0),\n",
       " Row(Ship_name='Conquest', Cruise_line='Carnival', Age=11, Tonnage=110.0, passengers=29.74, length=9.53, cabins=14.88, passenger_density=36.99, crew=19.1, cruise_cat=1.0),\n",
       " Row(Ship_name='Destiny', Cruise_line='Carnival', Age=17, Tonnage=101.353, passengers=26.42, length=8.92, cabins=13.21, passenger_density=38.36, crew=10.0, cruise_cat=1.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol = 'Cruise_line',outputCol = 'cruise_cat')\n",
    "indexed = indexer.fit(cruise_ship_info_df).transform(cruise_ship_info_df)\n",
    "indexed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5967c2c-6da3-4cf7-afc2-6677390bf657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "feba7ec0-41c3-40b7-80a6-f87125c2af7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship_name',\n",
       " 'Cruise_line',\n",
       " 'Age',\n",
       " 'Tonnage',\n",
       " 'passengers',\n",
       " 'length',\n",
       " 'cabins',\n",
       " 'passenger_density',\n",
       " 'crew',\n",
       " 'cruise_cat']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5758e656-5a9c-4d98-961f-89404d6720dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = ['Age',\n",
    " 'Tonnage',\n",
    " 'passengers',\n",
    " 'length',\n",
    " 'cabins',\n",
    " 'passenger_density',\n",
    " 'cruise_cat'], outputCol = 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2aa8ffc-9d3a-4090-b305-f018b81fc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68e0363a-8325-4c1a-872c-cb8c5832832e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ship_name',\n",
       " 'Cruise_line',\n",
       " 'Age',\n",
       " 'Tonnage',\n",
       " 'passengers',\n",
       " 'length',\n",
       " 'cabins',\n",
       " 'passenger_density',\n",
       " 'crew',\n",
       " 'cruise_cat',\n",
       " 'features']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5dd3a467-a1df-4abb-addd-4ba5a8bd9e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+----+\n",
      "|features                                          |crew|\n",
      "+--------------------------------------------------+----+\n",
      "|[6.0,30.276999999999997,6.94,5.94,3.55,42.64,16.0]|3.55|\n",
      "|[6.0,30.276999999999997,6.94,5.94,3.55,42.64,16.0]|3.55|\n",
      "|[26.0,47.262,14.86,7.22,7.43,31.8,1.0]            |6.7 |\n",
      "|[11.0,110.0,29.74,9.53,14.88,36.99,1.0]           |19.1|\n",
      "|[17.0,101.353,26.42,8.92,13.21,38.36,1.0]         |10.0|\n",
      "|[22.0,70.367,20.52,8.55,10.2,34.29,1.0]           |9.2 |\n",
      "|[15.0,70.367,20.52,8.55,10.2,34.29,1.0]           |9.2 |\n",
      "|[23.0,70.367,20.56,8.55,10.22,34.23,1.0]          |9.2 |\n",
      "|[19.0,70.367,20.52,8.55,10.2,34.29,1.0]           |9.2 |\n",
      "|[6.0,110.23899999999999,37.0,9.51,14.87,29.79,1.0]|11.5|\n",
      "|[10.0,110.0,29.74,9.51,14.87,36.99,1.0]           |11.6|\n",
      "|[28.0,46.052,14.52,7.27,7.26,31.72,1.0]           |6.6 |\n",
      "|[18.0,70.367,20.52,8.55,10.2,34.29,1.0]           |9.2 |\n",
      "|[17.0,70.367,20.52,8.55,10.2,34.29,1.0]           |9.2 |\n",
      "|[11.0,86.0,21.24,9.63,10.62,40.49,1.0]            |9.3 |\n",
      "|[8.0,110.0,29.74,9.51,14.87,36.99,1.0]            |11.6|\n",
      "|[9.0,88.5,21.24,9.63,10.62,41.67,1.0]             |10.3|\n",
      "|[15.0,70.367,20.52,8.55,10.2,34.29,1.0]           |9.2 |\n",
      "|[12.0,88.5,21.24,9.63,11.62,41.67,1.0]            |9.3 |\n",
      "|[20.0,70.367,20.52,8.55,10.2,34.29,1.0]           |9.2 |\n",
      "+--------------------------------------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select('features','crew').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1ae531e-217e-49fa-a2e0-dc80af4d98f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = output.select('features','crew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e99dda5-8cbd-4cd6-aa6e-10f807677a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "480fbe46-160c-4592-8a9f-6bad5d3cbfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c75f088-2c1b-4253-9ee2-fcbc08077ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8aa41f60-40ca-445e-ac0c-92bb120b948e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build regression model\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "24d8c80e-1997-4b71-a0f7-73dee19e9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_lr = LinearRegression(labelCol ='crew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e92158f7-7003-4e7e-a25f-9377dca322cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ship_model = ship_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f936683-c94b-4e66-ae47-a8b3c79b05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_results = trained_ship_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66aef765-7b70-445e-a95f-0a3d8c06ea01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65589385972916"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a00ff67c-2549-4071-a57a-3c523866b36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9632693662297822"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "569b2d9d-af8e-4aae-b851-603630d37d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49946735640322437"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_results.meanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27f25dd7-f9c0-4bfc-b03f-b83c85496a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37dc7c0f-5b19-4553-bc3d-8435454637f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>corr(crew, passengers)</th></tr>\n",
       "<tr><td>0.9152341306065384</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+----------------------+\n",
       "|corr(crew, passengers)|\n",
       "+----------------------+\n",
       "|    0.9152341306065384|\n",
       "+----------------------+"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## find coorelation - if cre size is related to passenger count\n",
    "cruise_ship_info_df.select(corr('crew','passengers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a572790-5ee6-4308-bef8-19c56a26955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Agression model steps \n",
    "#Step :1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
