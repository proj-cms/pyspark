PySpark note :

1. split and explode are column operations. So these operations can not be called on Df. To make use of these operations
   withColumn is important. And then use the newly created column in subsequent steps.
2. There are few important differences but the fundamental one is what happens with lineage. Persist / cache keeps lineage intact while checkpoint breaks lineage.
   Lets consider following examples:

3. The point is that each time you apply a transformation or perform a query on a data frame, the query plan grows. Spark keeps all history of transformations applied on a data frame that can be seen when run explain command on the data frame. When the query plan starts to be huge, the performance decreases dramatically, generating bottlenecks

4. Get number of partitions : employee_schema.rdd.getNumPartitions()

5. Performance improvment techniques : https://towardsdatascience.com/apache-spark-performance-boosting-e072a3ec1179
    -- join by broadcast
    -- Replace Joins & Aggregations with Windows
    -- Minimize Shuffles
       -- bucketing using bucketBy in DataFramWriter
       -- use proper join algorithm - SortMerge / HashJoin (shufffle has join / broadcast join )
    -- Cache Properly
       -- Cache
       -- unpersist

    -- Break the Lineage — Checkpointing
       -- df.

    -- Tackle with Skew Data — salting & repartition
      -- salting is technique for adding a value to existing data to create uniform distribution
      -- coalese/repartition
   -- Utilize Proper File Formats — Parquet

   -- Put the bigger dataset on the left in joins. : https://medium.com/softwaresanders/improving-your-apache-spark-application-performance-ec5d64bc3c75

   -- In Spark 3.0, significant improvements are achieved to tackle performance issues by Adaptive Query Execution, take upgrading the version into consideration
      -- https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html

   -- The physical plan is read from the bottom up, whereas the DAG is read from the top down

   -- Serialization also plays an important role in the performance of any distributed application. Formats that are slow to serialize objects into, or consume a large number of bytes, will greatly slow down the computation. For Scala/Java-based Spark applications, Kryo serialization is highly recommended. In Pyspark, Marshal and Pickle serializers are supported, MarshalSerializer is faster than PickleSerializer but supports fewer data types

   -- To resolve the problem of data skew, we can either: https://luminousmen.com/post/spark-tips-partition-tuning

      Repartition data on a more evenly distributed key if you have such.
      Broadcast the smaller dataframe if possible
      Split data into skewed and non-skewed data and work with them in parallel by redistributing skewed data (differential replication)
      Use an additional random key for better distribution of the data(salting).
      Iterative broadcast join
      More complicated methods that I have never used in my life.

  -- Use stage barrier between shuffle stage and the write operation -- localCheckpoint()

  df.localCheckpoint(...) \
    .repartition(n) \
    .write(...)

  This is very helpful as it breaks the stage barrier so coalesce or repartition will not go up your execution pipeline or the parallel workflow that uses the same dataframe does not need to re-process the current dataframe. Remember Spark is lazy execute, localCheckpoint() will trigger execution to materialize the dataframe

  -- what is differential replication ?

  -- what is leftanti join ?  : https://luminousmen.com/post/spark-tips-partition-tuning

  -- what is cross join ? .crossJoin(high)

  -- what is freqItems() method ?

  -- We can diagnose skewness by looking at the Spark UI and checking the time spent on each task. Some tasks are time-consuming, while others are not. Also, in the Spark UI, an indicator of data skew can be the huge difference between the minimum task time and the maximum task time

  https://medium.com/@achilleus/https-medium-com-joins-in-apache-spark-part-1-ce289bfc84c9
  -- joins in spark
     -- inner
     -- left outer
     -- right outer
     -- full outer join
     -- cross joins
     -- left-semi
     -- leftanti
     -- self

  -- study more about joining algorithm

  https://towardsdatascience.com/about-joins-in-spark-3-0-1e0ea083ea86

  -- understand more on adaptive qry engine

  https://databricks.com/blog/2020/05/29/adaptive-query-execution-speeding-up-spark-sql-at-runtime.html
  
  -- BucketBy method is available only when invoifing write methodo on dataframe.
  
  --

  

